---
title: Predicting Wine Quality and Wine Type
author: Eric Leung
date: 2016 June 22nd
output: 
  pdf_document: 
    fig_caption: yes
    fig_crop: no
    fig_height: 4
    fig_width: 5
    toc: yes
fontsize: 12pt
bibliography: refs.bib
classoption: twocolumn
---

```{r Knitr Options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```

```{r Figure Titles}
figure1 <- paste("Heatmap of variable correlations for the red wine data.",
                 "There are noteworthy correlations between: fixed acidity",
                 "and citric acid; density and fixed acidity; and pH and",
                 "fixed acidity.",
                 "Overall, not very many strong correlations.")
figure2 <- paste("Heatmap of variable correlations for the white wine data",
                 "There are noteworthy correlations between density and",
                 "alcohol as well as residual sugar and density.")
figure3 <- paste("Heatmap of variable correlations for all wine data",
                 "After combining the two data sets, the correlation matrix",
                 "is less sparse and there appears to be strong correlations.",
                 "These correlations should be kept in mind while continuing",
                 "with the analysis.")
compareFixed <- paste("Density distributions of fixed acidity in white and red",
                      "wine. Here we see a strong overlap between the two",
                      "wines. This suggests that using fixed acidity to",
                      "differentiate between wines will be difficult.")
compareChloride <- "Density distributions of chlorides in white and red wine"
figure4 <- "Distribution of pH values for all wine data"
figure5 <- "Distribution of citric acid values for all wine data"
figure6 <- "Distribution of raw chloride values for all wine data"
figure7 <- paste("Distribution of the natural log transformation of chloride",
                 "values for all wine data")
figList <- c("figure1", "figure2", "figure3", "compareFixed", "compareChloride",
             "figure4", "figure5", "figure6", "figure7", "dtPlot")
```

```{r Source Extenal Code}
source("../bin/splitdf.R")
source("../bin/naive-bayes.R")
source("../bin/decision-trees.R")
source("../bin/svm.R")
```

```{r Load Packages}
# Load packages
require(ggplot2)
require(dplyr)
require(corrplot)
require(cvTools)
```

```{r Source}
# Load data
red <- read.table(file = "../raw-data/winequality-red.csv",
                  header = TRUE,
                  stringsAsFactors = FALSE,
                  sep = ";") %>% tbl_df()
white <- read.table(file = "../raw-data/winequality-white.csv",
                    header = TRUE,
                    stringsAsFactors = FALSE,
                    sep = ";") %>% tbl_df()
```

# Abstract

There are many different wines, each with varying degrees of quality. This
"quality" is difficult to measure because it is usually perceived by an
individual. Here, I explore various machine learning methods in differentiating
between red and white wines using measured physicochemical properties.
Additionally, I try to predict wine quality based on these same measured
properties. The methods I used are decision trees (a tree based method),
support vector machine (non-probabilistic binary classifier), and naive Bayes
(a probabilistic classifier). The prediction of wine type was accurate for all
three methods I used. Prediction of wine quality only did slightly better than
random. There will have to be more tuning of the methods to do better in quality
prediction.

# Introduction

```{r Format Data}
redType <- red %>% mutate(type = "red")
whiteType <- white %>% mutate(type = "white")
wines <- bind_rows(redType, whiteType)
randomizeRows <- sample(seq(1, nrow(wines)), nrow(wines))
wines <- wines[randomizeRows, ]
```


## Wine Dataset Description

I used the Wine Quality Data Set. This data comes from Dr. Paulo Cortez and
their pursuit to predict wine quality preferences [@cortez2009modeling]. The
data was downloaded from the UCI Machine Learning Repository [@lichman2013uci].

The data analyzed included two data sets, one related to red *vinho verde* and
the other related to white *vinho verde*. Each of the two data sets contain
twelve variables, one of which was the participant's quality score for that wine
[@lichman2013uci]. This analysis will use both data sets, either in one data set
or two separate data sets and focusing on one type of wine.


## Wine Characteristics

The red wine data set has `r nrow(red)` samples while the white wine data set
has `r nrow(white)` samples. It was not clear from the reference paper on how
they collected the data besides being tested "at the official certification
entity (CVRVV)" [@cortez2009modeling].

The available physicochemical variables are:

1. fixed acidity (g(tartaric acid)/dm^3^)
2. volatile acidity (g(acetic acid)/dm^3^)
3. citric acid (g/dm^3^)
4. residual sugar (g/dm^3^)
5. chlorides (g(sodium chloride)/dm^3^)
6. free sulfur dioxide (mg/dm^3^)
7. total sulfur dioxide (mg/dm^3^)
8. density (g/cm^3^)
9. pH
10. sulphates (g(potassium sulphate)/dm^3^)
11. alcohol (vol.%)

These variables will be used in the machine learning analysis.


# Exploratory Data Analysis

Before I go straight into implementing various machine learning algorithms,
I will take a look at the data. This will ensure there isn't anything too
anomalous. This can also affect how my results are interpreted.


## Correlations

One thing I'd like to look at are correlations between the attributes in the
data. Correlations between attributes can show redundant information and other
potential structural information about the data set.

```{r red, fig.cap=figure1}
redCorr <- redType %>% select(-type) %>% cor
redCorr %>% corrplot(type = "lower")
```

Figure `r which(figList == "figure1")` shows a heatmap of variable
correlations for the red wine data. There are noteworthy correlations between:
fixed acidity and citric acid; density and fixed acidity; and pH and fixed
acidity. Overall, not very many strong correlations.

```{r white, fig.cap=figure2}
whiteCorr <- whiteType %>% select(-type) %>% cor
whiteCorr %>% corrplot(type = "lower")
```

Figure `r which(figList == "figure2")` shows a heatmap of variable correlations
for the white wine data. There are noteworthy correlations between density and
alcohol as well as residual sugar and density. As with Figure
`r which(figList == "figure1")`, there don't appear to be too many strongly
correlated attributes to worry.

```{r wines, fig.cap=figure3}
winesCorr <- wines %>% select(-type) %>% cor
winesCorr %>% corrplot(type = "lower")
```

Figures `r which(figList == "figure1")`, `r which(figList == "figure2")`, and
`r which(figList == "figure3")` show heat maps generated from correlation
matrices from just white wine data, just red wine data, and the combination of
both wines into one.


## Comparisons Between White and Red

In this report, I will be differentiating between red and white wine types.
Firstly, I will need to combine the two separate wine data sets. Here I will
explore potential issues in combining the data.

When combining the data, I will explore the distribution of each of the wine
attributes for both wines are similar. This compatibility between data sets will
lessen any bias or inherent structure in the data. Any underlying structure in
the combined data may give a machine learning algorithm information on
predicting wine type not found in the attributes.

```{r Compare Red/White Fixed Acidity, fig.cap=compareFixed}
wines %>% ggplot(aes(fixed.acidity, color = type)) + geom_density() +
    xlab("Fixed Acidity") + ylab("Density") +
    guides(color = guide_legend("Type")) + theme_minimal()
```

There are attributes where the distributions of values are similar. Figure
`r which(figList == "compareFixed")` shows one example this. The figure shows
distributions of chlorides, separated by wine type. Here we see that both types
of wine share a similar distribution shape. This suggests that this attribute
will not be too helpful in differentiating between the wine types. There are
number of attributes with this pattern: fixed acidity, citric acid, residual
sugar, density, alcohol, quality.

```{r Compare Red/White Cloride, fig.cap=compareChloride}
wines %>% ggplot(aes(chlorides, color = type)) + geom_density() +
    xlab("Chlorides") + ylab("Density") +
    guides(color = guide_legend("Type")) + theme_minimal()
```

However, there are attributes where the distributions are different. An example
of this is shown in Figure `r which(figList == "compareChloride")`. This shows
that we can generally split wine types based on chloride content and this
observation could be exploited. There are number of attributes that have this
pattern: chlorides, volatile acidity, free sulfur dioxide (to a lesser extent),
total sulfur dioxide, pH (to a lesser extent), sulphates (to a lesser extent).

These observations in Figure `r which(figList == "compareFixed")` and Figure
`r which(figList == "compareChloride")` suggest that a learning algorithm will
not have too hard of a time to distinguish between wine type.


## Normality and Skew

I will now take a look at the data to see and note any unusual samples or
attributes. All of our attributes are numerical, except for the wine type. For
simplicity, we will assume and transform our data to as close to a normal
distribution as possible.

```{r Wines pH, fig.cap=figure4}
wines %>% ggplot(aes(pH)) + geom_histogram() + ylab("Count") + theme_minimal()
```

The variables pH and quality appear to be normal enough. Shown in Figure 4
is the distribution of pH values. The distribution appears good enough to be
normal. The values for quality follow a similar distribution.

```{r Wines Citric Acid, fig.cap=figure5}
wines %>% ggplot(aes(citric.acid)) + geom_histogram() +
    ylab("Count") + theme_minimal()
```

There are six attributes which have outliers. These attributes are citric acid
concentration, free sulfur dioxide concentration, total sulfur dioxide, sulfur
dioxide concentration, density, and sulphates concentration. Figure 5 shows a
plot of the citric acid distribution, whereby you can see the bulk of the
distribution and extreme values to the right. The other three attribute with
outliers have similar shapes. Despite having outliers, the overall distribution
of these attributes follows a normal distribution.

```{r Wines Chlorides, fig.cap=figure6}
wines %>% ggplot(aes(chlorides)) + geom_histogram() +
    ylab("Count") + theme_minimal()
```

```{r Wines log(Chlorides), fig.cap=figure7}
wines %>% ggplot(aes(chlorides %>% log)) + geom_histogram() +
    xlab("Chloride Content") + ylab("Count") + theme_minimal()
```

Lastly, there are four attributes which have enough skew in their distribution 
which warrant a transformation. These attributes are fixed acidity, volatile 
acidity, residual sugar content, and chlorides. Figure 6 shows the raw values 
for chloride. The distribution of chloride values is highly skews and will be 
logarithm transformed to take a more normal distribution. The result of a
natural logarithm transformation is shown in Figure 7.

```{r Normalize Data}
wines <- wines %>%
    mutate(fixed.acidity = log(fixed.acidity)) %>%
    mutate(volatile.acidity = log(volatile.acidity)) %>%
    mutate(residual.sugar = log(residual.sugar)) %>%
    mutate(chlorides = log(chlorides))
```


# Methods

## Decision Trees

A decision tree, in the machine learning sense, is a method that partitions
your data into "purer" groups. Your data is initially a group of heterogeneous
groups based on their response variable. Each partition of the sample space is
supposed to increase the "purity" of that partition so that if you were to
randomly select a sample, you would have a greater chance for one of the
response categories than the others.

Here I use the `rpart` package is run my decision tree analysis. According to
the authors, their package is a re-implementation of Brienman, Friedman, Olshen,
and Stone's classification and regression trees
[@rpart, @breiman1984classification].

There are two general steps in decision trees: growing the tree and pruning the
tree. Growing the tree is essentially training the method, where you start
partitioning the data. The pruning step is to prevent over-fitting. Pruning is
where you remove sections of your grown tree because those splits give little
useful information.


## Support Vector Machine (SVM)

Support vector machines are another method to do classification and regression
analysis. Support vector machines are commonly abbreviated as SVMs. These work
in a similar way to the decision tress where it partitions the data. In contrast
to decision trees, SVMs performs just one "partition."

The line that partitions your data is called the hyperplane. For a linear SVM,
this hyperplane is defined as

$$\overrightarrow{b} \cdot \overrightarrow{x} - b = 0$$

where $\overrightarrow{w}$ is the normal vector to the hyperplane and
$\overrightarrow{x}$ is a set of points.

Hyperplanes can be linear, as mentioned above, or they can have a non-linear
spin to it. Non-linear hyperplanes can be achieved by using different kernels.
Some common kernels are polynomial, Gaussian radial basis, and hyperbolic
tangent.

In my analysis, I will implement an SVM using the `e1071` R package [@e1071].
I only implement the linear kernel for simplicity.


## Naive Bayes Classifier

Unlike a support vector machine or decision tree, the Naive Bayes classifier is
a probabilistic classifier. A Naive Bayes classifier also cannot do regression.

This classifier takes advantage of the famous Bayes' Theorem

$$P(A|B) = \frac{P(A|B)P(B)}{P(A)}$$

whereby you provide a prior probability of an event happening.

The prior in a classification sense is the training set you provide. For
simplicity, I will assume each attribute follows a Gaussian normal distribution.
Thus, I can model the prior probability with the Gaussian distribution. All I
now need is to calculate the means and standard deviations for input parameters.

For my case with two wine types to classify, I would take each type of wine and
calculate for each attribute the mean and standard deviation.

Once you have these parameters, mean and standard deviation, you can use them in
the probability density definition of the normal distribution

$$f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\sigma^2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

# Results

## Predict Red and White Wine

I hypothesize that there are enough physicochemical differences between red and
white wines that a learning algorithm will be able to differentiate between the
two.

```{r Predict Red and White Wine, echo=FALSE, results='hide'}
# Decision tree analysis
dtResults <- run_decision_tree_type(wines, "type", "class")

# Naive Bayes analysis
nbResults <- run_naive_bayes(wines, "type")

# SVM analysis
svmResults <- run_svm_type(wines, "type")
```

A summary of the results can be found in Table 1. As you can see, the linear
support vector machine performs the best out of the three methods I used.

\begin{table}[]
\centering
\begin{tabular}{l|l|l}
Method         & Training Error & Test Error \\ \hline
Decision Trees & 1.867\%        & 1.723\%    \\
Naive Bayes    & 1.560\%        & 1.538\%    \\
SVM (Linear)   & 0.678\%        & 0.923\%    \\
\end{tabular}
\caption{Summary of machine learning methods on predicting wine type}
\end{table}

From this quick look there are few conclusions we can make. Each of the methods
have greater than 98% accuracy in differing between wine types. Decision trees
have a more powerful ensemble counterpart for more complicate problems. However,
that might not be necessary here.

Same goes for the support vector machine. Here I just used the linear kernel
without resorting to any more complex kernels.


### Decision Tree Classsification of Wine

For the decision tree, we can zoom in a little bit on what its learned.

```{r Decision Tree Plot, eval=FALSE}
png('dt-results.png')
dtResults$fitted %>% plot(uniform = TRUE)
text(dtResults$fitted, all = TRUE, cex = 1.0)
dev.off()
```

![Decision tree results plot](dt-results.png)

In figure `r which(figList == "dtPlot")`, we can see the decision analysis the
decision tree made while learning the data. We can see that the first slice of
the data was with total sulfur dioxide and then chloride content was next. In
total, it appears the decision tree only made use of four attributes from the
data, including total sulfur dioxide and chlorides. The remaining two attributes
are density and residual sugar content.


## Predict Wine Quality

Another question I wanted to ask was whether you could predict wine quality.
Here I only used decision trees and support vector machines. However, with the
support vector machines, I varied the kernel, ranging from the linear kernel I
used before, a radial basis kernel, and a polynomial kernel. The varying kernel
changes the boundary line and space in which the data is looked at. This allows
the support vector machine to deal with more complex data.

```{r Predict Wine Quality}
dtQuality <- run_decision_tree_type(wines, "quality", "class")
svmQualityLinear <- run_svm_type(wines, "quality", kernel = "linear")
svmQualityRadial <- run_svm_type(wines, "quality", kernel = "radial")
svmQualityPoly <- run_svm_type(wines, "quality", kernel = "polynomial")
```

A summary of the results can be found in Table 2. The radial kernel SVM
performed the best on both the training and test set.

\begin{table}[]
\centering
\begin{tabular}{l|l|l}
Method         & Training Error & Test Error \\ \hline
Decision Trees & 46.367\%       & 46.031\%   \\
SVM (Linear)   & 46.367\%       & 46.708\%   \\
SVM (Radial)   & 38.875\%       & 42.769\%   \\
SVM (Poly)     & 39.984\%       & 43.692\%  
\end{tabular}
\caption{Summary of machine learning methods on predicting wine quality}
\end{table}

# Discussion and Conclusion

Wine value is determined by physicochemical properties and personal taste.
Physicochemical properties are determined and measured. Personal taste is a bit
more variable. Machine learning algorithms are now used to try and help predict
personal taste preferences.

Here I have tried to predict personal taste and wine type. Wine type is a binary
variable. Meanwhile, quality had a range between between 1 and 10. Here, I
performed classification on both wine type and wine quality.

The authors of the paper who generate the data made use of more regression
methods in prediction. Their rationale was that regression preserves the order
and semantic difference between a wine rated at four versus one at a nine.

For binary cases, it appears that decision trees, naive Bayes, and SVMs all
perform well. However, the multi-classification problem of predicting perceived
quality was much more difficult. 

Future work on this would be to try out more regression methods on wine quality.
Along with regressions, it may be worthwhile to explore interactions between
the physicochemical properties. For the current SVMs, some parameters to tune
are the cost for violating the margin boundary, tolerance, and shrinking.

# References